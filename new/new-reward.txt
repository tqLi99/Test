import math
import random
import time


import numpy as np
import rospy
import sensor_msgs.point_cloud2 as pc2
from gazebo_msgs.msg import ModelState
from geometry_msgs.msg import Twist
from nav_msgs.msg import Odometry
from sensor_msgs.msg import PointCloud2
from squaternion import Quaternion
from std_srvs.srv import Empty
from visualization_msgs.msg import Marker
from visualization_msgs.msg import MarkerArray

GOAL_REACHED_DIST = 0.3
COLLISION_DIST = 0.35
TIME_DELTA = 0.1


# Check if the random goal position is located on an obstacle and do not accept it if it is
def check_pos(x, y):
    goal_ok = True

    if -3.8 > x > -6.2 and 6.2 > y > 3.8:
        goal_ok = False

    if -1.3 > x > -2.7 and 4.7 > y > -0.2:
        goal_ok = False

    if -0.3 > x > -4.2 and 2.7 > y > 1.3:
        goal_ok = False

    if -0.8 > x > -4.2 and -2.3 > y > -4.2:
        goal_ok = False

    if -1.3 > x > -3.7 and -0.8 > y > -2.7:
        goal_ok = False

    if 4.2 > x > 0.8 and -1.8 > y > -3.2:
        goal_ok = False

    if 4 > x > 2.5 and 0.7 > y > -3.2:
        goal_ok = False

    if 6.2 > x > 3.8 and -3.3 > y > -4.2:
        goal_ok = False

    if 4.2 > x > 1.3 and 3.7 > y > 1.5:
        goal_ok = False

    if -3.0 > x > -7.2 and 0.5 > y > -1.5:
        goal_ok = False

    if x > 4.5 or x < -4.5 or y > 4.5 or y < -4.5:
        goal_ok = False

    return goal_ok


class ImprovedPotentialField:
    """改进的势能场计算类"""
    def __init__(self, 
                 attractive_gain=2.0,      # 吸引势能增益
                 repulsive_gain=1.0,       # 排斥势能增益
                 attractive_decay=2.0,     # 吸引势场衰减率
                 goal_threshold=0.3,       # 目标阈值
                 obstacle_threshold=1.5,   # 障碍物影响阈值
                 max_potential=10.0,       # 势能上限
                 min_safe_dist=0.4,        # 最小安全距离
                 approach_factor=1.2,      # 靠近目标的势能因子
                 retreat_factor=1.5):      # 远离目标的势能因子
        
        self.attractive_gain = attractive_gain
        self.repulsive_gain = repulsive_gain
        self.attractive_decay = attractive_decay
        self.goal_threshold = goal_threshold
        self.obstacle_threshold = obstacle_threshold
        self.max_potential = max_potential
        self.min_safe_dist = min_safe_dist
        self.approach_factor = approach_factor  # 靠近目标时增强势能差异
        self.retreat_factor = retreat_factor    # 远离目标时增强势能差异
    
    def exponential_attractive_potential(self, distance):
        """使用指数形式的吸引势能函数，提供平滑且非线性的梯度"""
        if distance < self.goal_threshold:
            return 0.0
        # 使用更陡峭的指数曲线增强吸引力
        return self.attractive_gain * (1 - np.exp(-self.attractive_decay * distance))
    
    def repulsive_potential(self, min_obstacle_distance):
        """改进的排斥势能函数，使用更平滑的函数形式"""
        if min_obstacle_distance >= self.obstacle_threshold:
            return 0.0
        
        # 使用平滑过渡的排斥势能函数
        if min_obstacle_distance <= self.min_safe_dist:
            # 在安全距离内，势能最大
            return self.repulsive_gain
        else:
            # 安全距离外，势能平滑衰减
            normalized_dist = (min_obstacle_distance - self.min_safe_dist) / (self.obstacle_threshold - self.min_safe_dist)
            return self.repulsive_gain * (1 - normalized_dist) ** 2
    
    def calculate_potential(self, current_pos, goal_pos, min_obstacle_dist):
        """计算总势能"""
        # 计算到目标的距离
        goal_distance = np.linalg.norm([current_pos[0] - goal_pos[0], current_pos[1] - goal_pos[1]])
        
        # 使用指数形式的吸引势能
        attractive = self.exponential_attractive_potential(goal_distance)
        
        # 计算排斥势能
        repulsive = self.repulsive_potential(min_obstacle_dist)
        
        # 计算总势能 (吸引势能 + 排斥势能)
        total_potential = attractive + repulsive
        
        # 限制势能范围
        return np.clip(total_potential, 0.0, self.max_potential)
    
    def calculate_potential_difference(self, current_state, next_state, goal_pos, current_min_obstacle, next_min_obstacle, gamma=0.99):
        """计算两个状态之间的势能差异，并根据靠近或远离目标增强差异"""
        # 计算当前和下一个状态的势能
        current_potential = self.calculate_potential(current_state, goal_pos, current_min_obstacle)
        next_potential = self.calculate_potential(next_state, goal_pos, next_min_obstacle)
        
        # 计算当前和下一状态到目标的距离
        current_distance = np.linalg.norm([current_state[0] - goal_pos[0], current_state[1] - goal_pos[1]])
        next_distance = np.linalg.norm([next_state[0] - goal_pos[0], next_state[1] - goal_pos[1]])
        
        # 势能差异的基础计算
        potential_diff = current_potential - gamma * next_potential
        
        # 根据距离变化调整势能差异
        if next_distance < current_distance:  # 靠近目标
            # 增强靠近目标的正向奖励
            potential_diff *= self.approach_factor
        else:  # 远离目标
            # 增强远离目标的负向惩罚
            potential_diff *= self.retreat_factor
            
        return potential_diff


class GazeboEnv:
    """Superclass for all Gazebo environments."""

    def __init__(self, launchfile, environment_dim):
        self.environment_dim = environment_dim
        self.odom_x = 0
        self.odom_y = 0
        self.prev_odom_x = 0  # 添加前一时刻位置
        self.prev_odom_y = 0  # 添加前一时刻位置

        self.goal_x = 1
        self.goal_y = 0.0

        self.upper = 5.0
        self.lower = -5.0
        self.velodyne_data = np.ones(self.environment_dim) * 10
        self.last_odom = None
        self.odom_received = False  # 添加标志，指示是否收到了odom消息
        self.last_action = [0.0, 0.0]  # 存储上一次的动作

        # 势能函数参数
        self.gamma = 0.99  # 势能折扣因子
        # 奖励权重参数
        self.speed_weight = 0.5        # 速度奖励权重
        self.turn_weight = 0.8         # 转向惩罚权重  
        self.obstacle_weight = 0.5     # 障碍物惩罚权重
        self.potential_weight = 2.5    # 增强势能奖励权重
        self.smoothness_weight = 0.2   # 平滑奖励权重
        
        # 设置转向惩罚阈值和系数
        self.excessive_turn_threshold = 0.5  # 过度转向的阈值
        self.excessive_turn_penalty = 1.0    # 过度转向的惩罚系数
        
        # 目标和碰撞阈值
        self.goal_threshold = GOAL_REACHED_DIST
        self.collision_threshold = COLLISION_DIST

        self.set_self_state = ModelState()
        self.set_self_state.model_name = "p3dx"
        self.set_self_state.pose.position.x = 0.0
        self.set_self_state.pose.position.y = 0.0
        self.set_self_state.pose.position.z = 0.0
        self.set_self_state.pose.orientation.x = 0.0
        self.set_self_state.pose.orientation.y = 0.0
        self.set_self_state.pose.orientation.z = 0.0
        self.set_self_state.pose.orientation.w = 1.0

        self.gaps = [[-np.pi / 2 - 0.03, -np.pi / 2 + np.pi / self.environment_dim]]
        for m in range(self.environment_dim - 1):
            self.gaps.append(
                [self.gaps[m][1], self.gaps[m][1] + np.pi / self.environment_dim]
            )
        self.gaps[-1][-1] += 0.03

        print("Roscore launched!")

        # Launch the simulation with the given launchfile name
        rospy.init_node("gym", anonymous=True)

        # Set up the ROS publishers and subscribers
        self.vel_pub = rospy.Publisher("p3dx/cmd_vel", Twist, queue_size=1)
        self.set_state = rospy.Publisher(
            "gazebo/set_model_state", ModelState, queue_size=10
        )
        self.unpause = rospy.ServiceProxy("/gazebo/unpause_physics", Empty)
        self.pause = rospy.ServiceProxy("/gazebo/pause_physics", Empty)
        self.reset_proxy = rospy.ServiceProxy("/gazebo/reset_world", Empty)
        self.publisher = rospy.Publisher("goal_point", MarkerArray, queue_size=3)
        self.publisher2 = rospy.Publisher("linear_velocity", MarkerArray, queue_size=1)
        self.publisher3 = rospy.Publisher("angular_velocity", MarkerArray, queue_size=1)
        self.velodyne = rospy.Subscriber(
            "/velodyne_points", PointCloud2, self.velodyne_callback, queue_size=1
        )
        self.odom = rospy.Subscriber(
            "/p3dx/odom", Odometry, self.odom_callback, queue_size=1
        )
        
        # 等待接收第一个odom消息 - 修复self.last_odom为None的问题
        print("Waiting for first odometry message...")
        timeout = 10.0  # 设置10秒超时
        start_time = time.time()
        while not self.odom_received and time.time() - start_time < timeout:
            time.sleep(0.1)
        
        if not self.odom_received:
            print("Warning: No odometry message received within timeout!")
            # 创建默认的odom消息
            self.last_odom = Odometry()
            self.last_odom.pose.pose.position.x = 0.0
            self.last_odom.pose.pose.position.y = 0.0
            self.last_odom.pose.pose.orientation.w = 1.0
        
        # 初始化势能场计算器，调整参数
        self.potential_field = ImprovedPotentialField(
            attractive_gain=2.5,        # 增强吸引势能增益 
            repulsive_gain=1.2,         # 适当增加排斥势能增益
            attractive_decay=2.0,       # 增加吸引势场衰减率
            obstacle_threshold=1.5,
            min_safe_dist=0.4,
            approach_factor=1.5,        # 靠近目标时的势能因子
            retreat_factor=2.0          # 远离目标时的势能因子（惩罚更强）
        )
        
        print("Environment initialization complete!")

    # Read velodyne pointcloud and turn it into distance data, then select the minimum value for each angle
    # range as state representation
    def velodyne_callback(self, v):
        data = list(pc2.read_points(v, skip_nans=False, field_names=("x", "y", "z")))
        self.velodyne_data = np.ones(self.environment_dim) * 10
        for i in range(len(data)):
            if data[i][2] > -0.2:
                dot = data[i][0] * 1 + data[i][1] * 0
                mag1 = math.sqrt(math.pow(data[i][0], 2) + math.pow(data[i][1], 2))
                mag2 = math.sqrt(math.pow(1, 2) + math.pow(0, 2))
                beta = math.acos(dot / (mag1 * mag2)) * np.sign(data[i][1])
                dist = math.sqrt(data[i][0] ** 2 + data[i][1] ** 2 + data[i][2] ** 2)

                for j in range(len(self.gaps)):
                    if self.gaps[j][0] <= beta < self.gaps[j][1]:
                        self.velodyne_data[j] = min(self.velodyne_data[j], dist)
                        break

    def odom_callback(self, od_data):
        self.last_odom = od_data
        self.odom_received = True
        
    # 基于势能场的奖励函数 - 完全基于势能差异来奖励/惩罚靠近/远离目标行为
    def potential_based_reward(self, target, collision, action, current_state, next_state, current_min_laser, next_min_laser):
        """
        完全基于势能场的奖励函数 - 通过势能差异表达靠近/远离目标的奖励/惩罚
        """
        # 基础奖励 - 目标达成和碰撞处理
        if target:
            return 100.0
        elif collision:
            return -100.0
        
        # 使用势能场类计算势能差异（包含了靠近目标的奖励和远离目标的惩罚）
        potential_diff = self.potential_field.calculate_potential_difference(
            current_state, 
            next_state, 
            (self.goal_x, self.goal_y), 
            current_min_laser,
            next_min_laser,
            self.gamma
        )
        
        # 计算过度转弯惩罚
        turn_penalty = 0
        angular_velocity = abs(action[1])
        if angular_velocity > self.excessive_turn_threshold:
            # 使用二次函数使惩罚随转向增大而更快增长
            turn_penalty = -self.turn_weight * (angular_velocity - self.excessive_turn_threshold) ** 2
        
        # 平滑性惩罚 - 惩罚动作与上一动作之间的突变
        action_change = np.abs(np.array(action) - np.array(self.last_action))
        smoothness_penalty = -self.smoothness_weight * np.sum(action_change)
        
        # 各奖励组件
        rewards = {
            # 1. 势能差奖励 (主要引导信号，已经包含了靠近/远离目标的奖励/惩罚)
            'potential_reward': potential_diff * self.potential_weight,
            
            # 2. 过度转弯惩罚
            'turn_penalty': turn_penalty,
            
            # 3. 行为平滑性惩罚
            'smoothness_penalty': smoothness_penalty,
            
            # 4. 存活惩罚 (鼓励快速完成任务)
            'living_penalty': -0.05
        }
        
        # 更新上一次的动作
        self.last_action = action.copy()
        
        # 组合所有奖励分量
        total_reward = sum(rewards.values())
        
        # 限制奖励范围
        return np.clip(total_reward, -10.0, 10.0)

    # Perform an action and read a new state - 使用基于势能场的奖励函数
    def step(self, action):
        target = False

        # 保存当前位置和激光数据作为前一个状态
        current_state = (self.odom_x, self.odom_y)
        current_min_laser = min(self.velodyne_data)
        self.prev_odom_x = self.odom_x
        self.prev_odom_y = self.odom_y

        # Publish the robot action
        vel_cmd = Twist()
        vel_cmd.linear.x = action[0]
        vel_cmd.angular.z = action[1]
        self.vel_pub.publish(vel_cmd)
        self.publish_markers(action)

        rospy.wait_for_service("/gazebo/unpause_physics")
        try:
            self.unpause()
        except (rospy.ServiceException) as e:
            print("/gazebo/unpause_physics service call failed")

        # propagate state for TIME_DELTA seconds
        time.sleep(TIME_DELTA)

        rospy.wait_for_service("/gazebo/pause_physics")
        try:
            pass
            self.pause()
        except (rospy.ServiceException) as e:
            print("/gazebo/pause_physics service call failed")

        # read velodyne laser state
        done, collision, next_min_laser = self.observe_collision(self.velodyne_data)
        v_state = []
        v_state[:] = self.velodyne_data[:]
        laser_state = [v_state]

        # Calculate robot heading from odometry data (防御性编程，确保last_odom不为None)
        if self.last_odom is None:
            print("Warning: No odometry data available, using default values")
            self.last_odom = Odometry()
            self.last_odom.pose.pose.position.x = self.odom_x
            self.last_odom.pose.pose.position.y = self.odom_y
            self.last_odom.pose.pose.orientation.w = 1.0
            
        self.odom_x = self.last_odom.pose.pose.position.x
        self.odom_y = self.last_odom.pose.pose.position.y
        next_state = (self.odom_x, self.odom_y)
        
        quaternion = Quaternion(
            self.last_odom.pose.pose.orientation.w,
            self.last_odom.pose.pose.orientation.x,
            self.last_odom.pose.pose.orientation.y,
            self.last_odom.pose.pose.orientation.z,
        )
        euler = quaternion.to_euler(degrees=False)
        angle = round(euler[2], 4)

        # Calculate distance to the goal from the robot
        distance = np.linalg.norm(
            [self.odom_x - self.goal_x, self.odom_y - self.goal_y]
        )

        # Calculate the relative angle between the robots heading and heading toward the goal
        skew_x = self.goal_x - self.odom_x
        skew_y = self.goal_y - self.odom_y
        dot = skew_x * 1 + skew_y * 0
        mag1 = math.sqrt(math.pow(skew_x, 2) + math.pow(skew_y, 2))
        mag2 = math.sqrt(math.pow(1, 2) + math.pow(0, 2))
        beta = math.acos(dot / (mag1 * mag2))
        if skew_y < 0:
            if skew_x < 0:
                beta = -beta
            else:
                beta = 0 - beta
        theta = beta - angle
        if theta > np.pi:
            theta = np.pi - theta
            theta = -np.pi - theta
        if theta < -np.pi:
            theta = -np.pi - theta
            theta = np.pi - theta

        # Detect if the goal has been reached and give a large positive reward
        if distance < GOAL_REACHED_DIST:
            target = True
            done = True

        robot_state = [distance, theta, action[0], action[1]]
        state = np.append(laser_state, robot_state)
        
        # 使用基于势能场的奖励函数计算奖励
        reward = self.potential_based_reward(
            target, collision, action, 
            current_state, next_state,
            current_min_laser, next_min_laser
        )
        
        return state, reward, done, target

    def reset(self):
        # Resets the state of the environment and returns an initial observation.
        rospy.wait_for_service("/gazebo/reset_world")
        try:
            self.reset_proxy()

        except rospy.ServiceException as e:
            print("/gazebo/reset_simulation service call failed")

        angle = np.random.uniform(-np.pi, np.pi)
        quaternion = Quaternion.from_euler(0.0, 0.0, angle)
        object_state = self.set_self_state

        x = 0
        y = 0
        position_ok = False
        while not position_ok:
            x = np.random.uniform(-4.5, 4.5)
            y = np.random.uniform(-4.5, 4.5)
            position_ok = check_pos(x, y)
        object_state.pose.position.x = x
        object_state.pose.position.y = y
        # object_state.pose.position.z = 0.
        object_state.pose.orientation.x = quaternion.x
        object_state.pose.orientation.y = quaternion.y
        object_state.pose.orientation.z = quaternion.z
        object_state.pose.orientation.w = quaternion.w
        self.set_state.publish(object_state)

        self.odom_x = object_state.pose.position.x
        self.odom_y = object_state.pose.position.y
        self.prev_odom_x = self.odom_x
        self.prev_odom_y = self.odom_y
        self.last_action = [0.0, 0.0]  # 重置上一次动作

        # set a random goal in empty space in environment
        self.change_goal()
        # randomly scatter boxes in the environment
        self.random_box()
        self.publish_markers([0.0, 0.0])

        rospy.wait_for_service("/gazebo/unpause_physics")
        try:
            self.unpause()
        except (rospy.ServiceException) as e:
            print("/gazebo/unpause_physics service call failed")

        time.sleep(TIME_DELTA)

        rospy.wait_for_service("/gazebo/pause_physics")
        try:
            self.pause()
        except (rospy.ServiceException) as e:
            print("/gazebo/pause_physics service call failed")
        v_state = []
        v_state[:] = self.velodyne_data[:]
        laser_state = [v_state]

        distance = np.linalg.norm(
            [self.odom_x - self.goal_x, self.odom_y - self.goal_y]
        )

        skew_x = self.goal_x - self.odom_x
        skew_y = self.goal_y - self.odom_y

        dot = skew_x * 1 + skew_y * 0
        mag1 = math.sqrt(math.pow(skew_x, 2) + math.pow(skew_y, 2))
        mag2 = math.sqrt(math.pow(1, 2) + math.pow(0, 2))
        beta = math.acos(dot / (mag1 * mag2))

        if skew_y < 0:
            if skew_x < 0:
                beta = -beta
            else:
                beta = 0 - beta
        theta = beta - angle

        if theta > np.pi:
            theta = np.pi - theta
            theta = -np.pi - theta
        if theta < -np.pi:
            theta = -np.pi - theta
            theta = np.pi - theta

        robot_state = [distance, theta, 0.0, 0.0]
        state = np.append(laser_state, robot_state)
        return state

    def change_goal(self):
        # Place a new goal and check if its location is not on one of the obstacles
        if self.upper < 10:
            self.upper += 0.004
        if self.lower > -10:
            self.lower -= 0.004

        goal_ok = False

        while not goal_ok:
            self.goal_x = self.odom_x + random.uniform(self.upper, self.lower)
            self.goal_y = self.odom_y + random.uniform(self.upper, self.lower)
            goal_ok = check_pos(self.goal_x, self.goal_y)

    def random_box(self):
        # Randomly change the location of the boxes in the environment on each reset to randomize the training
        # environment
        for i in range(4):
            name = "cardboard_box_" + str(i)

            x = 0
            y = 0
            box_ok = False
            while not box_ok:
                x = np.random.uniform(-6, 6)
                y = np.random.uniform(-6, 6)
                box_ok = check_pos(x, y)
                distance_to_robot = np.linalg.norm([x - self.odom_x, y - self.odom_y])
                distance_to_goal = np.linalg.norm([x - self.goal_x, y - self.goal_y])
                if distance_to_robot < 1.5 or distance_to_goal < 1.5:
                    box_ok = False
            box_state = ModelState()
            box_state.model_name = name
            box_state.pose.position.x = x
            box_state.pose.position.y = y
            box_state.pose.position.z = 0.0
            box_state.pose.orientation.x = 0.0
            box_state.pose.orientation.y = 0.0
            box_state.pose.orientation.z = 0.0
            box_state.pose.orientation.w = 1.0
            self.set_state.publish(box_state)

    def publish_markers(self, action):
        # Publish visual data in Rviz
        markerArray = MarkerArray()
        marker = Marker()
        marker.header.frame_id = "odom"
        marker.type = marker.CYLINDER
        marker.action = marker.ADD
        marker.scale.x = 0.1
        marker.scale.y = 0.1
        marker.scale.z = 0.01
        marker.color.a = 1.0
        marker.color.r = 0.0
        marker.color.g = 1.0
        marker.color.b = 0.0
        marker.pose.orientation.w = 1.0
        marker.pose.position.x = self.goal_x
        marker.pose.position.y = self.goal_y
        marker.pose.position.z = 0

        markerArray.markers.append(marker)

        self.publisher.publish(markerArray)

        markerArray2 = MarkerArray()
        marker2 = Marker()
        marker2.header.frame_id = "odom"
        marker2.type = marker.CUBE
        marker2.action = marker.ADD
        marker2.scale.x = abs(action[0])
        marker2.scale.y = 0.1
        marker2.scale.z = 0.01
        marker2.color.a = 1.0
        marker2.color.r = 1.0
        marker2.color.g = 0.0
        marker2.color.b = 0.0
        marker2.pose.orientation.w = 1.0
        marker2.pose.position.x = 5
        marker2.pose.position.y = 0
        marker2.pose.position.z = 0

        markerArray2.markers.append(marker2)
        self.publisher2.publish(markerArray2)

        markerArray3 = MarkerArray()
        marker3 = Marker()
        marker3.header.frame_id = "odom"
        marker3.type = marker.CUBE
        marker3.action = marker.ADD
        marker3.scale.x = abs(action[1])
        marker3.scale.y = 0.1
        marker3.scale.z = 0.01
        marker3.color.a = 1.0
        marker3.color.r = 1.0
        marker3.color.g = 0.0
        marker3.color.b = 0.0
        marker3.pose.orientation.w = 1.0
        marker3.pose.position.x = 5
        marker3.pose.position.y = 0.2
        marker3.pose.position.z = 0

        markerArray3.markers.append(marker3)
        self.publisher3.publish(markerArray3)

    @staticmethod
    def observe_collision(laser_data):
        # Detect a collision from laser data
        min_laser = min(laser_data)
        if min_laser < COLLISION_DIST:
            return True, True, min_laser
        return False, False, min_laser
